{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Bot QA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de este notebook es utilizar los datos del challenge ConvAI2 (Conversational Intelligence Challenge 2) con conversaciones en inglés para desarrollar un chatbot capaz de responder preguntas de usuario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping c:\\Users\\Enzo\\Documents\\CEIA - UBA\\Procesamiento del Lenguaje Natural\\NLP_Challenges\\.venv\\Lib\\site-packages\\~-_dtypes-0.4.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping c:\\Users\\Enzo\\Documents\\CEIA - UBA\\Procesamiento del Lenguaje Natural\\NLP_Challenges\\.venv\\Lib\\site-packages\\~._dtypes-0.4.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping c:\\Users\\Enzo\\Documents\\CEIA - UBA\\Procesamiento del Lenguaje Natural\\NLP_Challenges\\.venv\\Lib\\site-packages\\~=_dtypes-0.4.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping c:\\Users\\Enzo\\Documents\\CEIA - UBA\\Procesamiento del Lenguaje Natural\\NLP_Challenges\\.venv\\Lib\\site-packages\\~l_dtypes-0.4.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping c:\\Users\\Enzo\\Documents\\CEIA - UBA\\Procesamiento del Lenguaje Natural\\NLP_Challenges\\.venv\\Lib\\site-packages\\~~_dtypes-0.4.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping c:\\Users\\Enzo\\Documents\\CEIA - UBA\\Procesamiento del Lenguaje Natural\\NLP_Challenges\\.venv\\Lib\\site-packages\\~-_dtypes-0.4.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping c:\\Users\\Enzo\\Documents\\CEIA - UBA\\Procesamiento del Lenguaje Natural\\NLP_Challenges\\.venv\\Lib\\site-packages\\~._dtypes-0.4.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping c:\\Users\\Enzo\\Documents\\CEIA - UBA\\Procesamiento del Lenguaje Natural\\NLP_Challenges\\.venv\\Lib\\site-packages\\~=_dtypes-0.4.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping c:\\Users\\Enzo\\Documents\\CEIA - UBA\\Procesamiento del Lenguaje Natural\\NLP_Challenges\\.venv\\Lib\\site-packages\\~l_dtypes-0.4.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping c:\\Users\\Enzo\\Documents\\CEIA - UBA\\Procesamiento del Lenguaje Natural\\NLP_Challenges\\.venv\\Lib\\site-packages\\~~_dtypes-0.4.0.dist-info due to invalid metadata entry 'name'\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gdown\n",
      "  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting beautifulsoup4 (from gdown)\n",
      "  Using cached beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting filelock (from gdown)\n",
      "  Downloading filelock-3.15.4-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting requests[socks] (from gdown)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\enzo\\documents\\ceia - uba\\procesamiento del lenguaje natural\\nlp_challenges\\.venv\\lib\\site-packages (from gdown) (4.66.2)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->gdown)\n",
      "  Using cached soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests[socks]->gdown)\n",
      "  Using cached charset_normalizer-3.3.2-cp312-cp312-win_amd64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests[socks]->gdown)\n",
      "  Using cached idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\enzo\\documents\\ceia - uba\\procesamiento del lenguaje natural\\nlp_challenges\\.venv\\lib\\site-packages (from requests[socks]->gdown) (2.2.1)\n",
      "Collecting certifi>=2017.4.17 (from requests[socks]->gdown)\n",
      "  Downloading certifi-2024.6.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6 (from requests[socks]->gdown)\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\enzo\\documents\\ceia - uba\\procesamiento del lenguaje natural\\nlp_challenges\\.venv\\lib\\site-packages (from tqdm->gdown) (0.4.6)\n",
      "Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Using cached beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "Downloading filelock-3.15.4-py3-none-any.whl (16 kB)\n",
      "Downloading certifi-2024.6.2-py3-none-any.whl (164 kB)\n",
      "   ---------------------------------------- 0.0/164.4 kB ? eta -:--:--\n",
      "   ---------------------- ----------------- 92.2/164.4 kB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 164.4/164.4 kB 3.3 MB/s eta 0:00:00\n",
      "Using cached charset_normalizer-3.3.2-cp312-cp312-win_amd64.whl (100 kB)\n",
      "Using cached idna-3.7-py3-none-any.whl (66 kB)\n",
      "Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Using cached soupsieve-2.5-py3-none-any.whl (36 kB)\n",
      "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "   ---------------------------------------- 0.0/64.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 64.9/64.9 kB 3.4 MB/s eta 0:00:00\n",
      "Installing collected packages: soupsieve, PySocks, idna, filelock, charset-normalizer, certifi, requests, beautifulsoup4, gdown\n",
      "Successfully installed PySocks-1.7.1 beautifulsoup4-4.12.3 certifi-2024.6.2 charset-normalizer-3.3.2 filelock-3.15.4 gdown-5.2.0 idna-3.7 requests-2.32.3 soupsieve-2.5\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1awUxYwImF84MIT5-jCaYAPe2QwSgS1hN&export=download\n",
      "To: c:\\Users\\Enzo\\Documents\\CEIA - UBA\\Procesamiento del Lenguaje Natural\\NLP_Challenges\\Challenge 4\\data_volunteers.json\n",
      "100%|██████████| 2.58M/2.58M [00:00<00:00, 6.44MB/s]\n"
     ]
    }
   ],
   "source": [
    "if os.access('data_volunteers.json', os.F_OK) is False:\n",
    "    url = 'https://drive.google.com/uc?id=1awUxYwImF84MIT5-jCaYAPe2QwSgS1hN&export=download'\n",
    "    output = 'data_volunteers.json'\n",
    "    gdown.download(url, output, quiet=False)\n",
    "else:\n",
    "    print('El dataset ya se encuentra descargado')\n",
    "\n",
    "text_file = 'data_volunteers.json'\n",
    "with open(text_file) as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['dialog', 'start_time', 'end_time', 'bot_profile', 'user_profile', 'eval_score', 'profile_match', 'participant1_id', 'participant2_id'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observar los campos disponibles en cada linea del dataset\n",
    "data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de rows utilizadas: 6033\n"
     ]
    }
   ],
   "source": [
    "chat_in = []\n",
    "chat_out = []\n",
    "\n",
    "input_sentences = []\n",
    "output_sentences = []\n",
    "output_sentences_inputs = []\n",
    "max_len = 30\n",
    "\n",
    "def clean_text(txt):\n",
    "    txt = txt.lower()    \n",
    "    txt.replace('\\'d', ' had')\n",
    "    txt.replace('\\'s', ' is')\n",
    "    txt.replace('\\'m', ' am')\n",
    "    txt.replace('don\\'t', 'do not')\n",
    "    txt = re.sub(r'\\W+', ' ', txt)\n",
    "    \n",
    "    return txt\n",
    "\n",
    "for line in data:\n",
    "    for i in range(len(line['dialog']) - 1):\n",
    "        # vamos separando el texto en \"preguntas\" (chat_in)\n",
    "        # y \"respuestas\" (chat_out)\n",
    "        chat_in = clean_text(line['dialog'][i]['text'])\n",
    "        chat_out = clean_text(line['dialog'][i + 1]['text'])\n",
    "\n",
    "        if len(chat_in) >= max_len or len(chat_out) >= max_len:\n",
    "            continue\n",
    "\n",
    "        input_sentence, output = chat_in, chat_out\n",
    "        \n",
    "        # output sentence (decoder_output) tiene <eos>\n",
    "        output_sentence = output + ' <eos>'\n",
    "        # output sentence input (decoder_input) tiene <sos>\n",
    "        output_sentence_input = '<sos> ' + output\n",
    "\n",
    "        input_sentences.append(input_sentence)\n",
    "        output_sentences.append(output_sentence)\n",
    "        output_sentences_inputs.append(output_sentence_input)\n",
    "\n",
    "print('Cantidad de rows utilizadas:', len(input_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('hi how are you ', 'not bad and you  <eos>', '<sos> not bad and you ')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sentences[1], output_sentences[1], output_sentences_inputs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras a índices:\n",
      "[('hello ', 5432), ('hi how are you ', 5763), ('hi ', 5638), ('where are you working ', 5), ('bro ', 6)]\n",
      "[('hi how are you  <eos>', 5367), ('not bad and you  <eos>', 1), ('hello  <eos>', 5882), ('hello how are you today  <eos>', 5807), ('bro  <eos>', 5118)]\n",
      "[('<sos> hi how are you ', 5367), ('<sos> not bad and you ', 1), ('<sos> hello ', 5882), ('<sos> hello how are you today ', 5807), ('<sos> bro ', 5118)]\n",
      "\n",
      "Índices a palabras:\n",
      "[(5432, 'hello '), (5763, 'hi how are you '), (5638, 'hi '), (5, 'where are you working '), (6, 'bro ')]\n",
      "[(5367, 'hi how are you  <eos>'), (1, 'not bad and you  <eos>'), (5882, 'hello  <eos>'), (5807, 'hello how are you today  <eos>'), (5118, 'bro  <eos>')]\n",
      "[(5367, '<sos> hi how are you '), (1, '<sos> not bad and you '), (5882, '<sos> hello '), (5807, '<sos> hello how are you today '), (5118, '<sos> bro ')]\n"
     ]
    }
   ],
   "source": [
    "# ESTA CELDA ESTÁ MAL. USAR TOKENIZER?\n",
    "\n",
    "# Diccionarios para mapear índices a palabras\n",
    "word2idx_inputs = {k: v for v, k in enumerate(input_sentences)}\n",
    "word2idx_outputs = {k: v for v, k in enumerate(output_sentences)}\n",
    "word2idx_outputs_inputs = {k: v for v, k in enumerate(output_sentences_inputs)}\n",
    "\n",
    "# Diccionarios para mapear palabras a índices\n",
    "idx2word_inputs = {v: k for k, v in word2idx_inputs.items()}\n",
    "idx2word_outputs = {v: k for k, v in word2idx_outputs.items()}\n",
    "idx2word__outputs_inputs = {v: k for k, v in word2idx_outputs_inputs.items()}\n",
    "\n",
    "print('Palabras a índices:')\n",
    "print(list(word2idx_inputs.items())[:5])\n",
    "print(list(word2idx_outputs.items())[:5])\n",
    "print(list(word2idx_outputs_inputs.items())[:5])\n",
    "\n",
    "print('\\nÍndices a palabras:')\n",
    "print(list(idx2word_inputs.items())[:5])\n",
    "print(list(idx2word_outputs.items())[:5])\n",
    "print(list(idx2word__outputs_inputs.items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder_input_sequences = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
