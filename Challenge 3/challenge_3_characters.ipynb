{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "4aPNVaEPuzdf",
        "qaRsRsb8ycYQ",
        "jgrT40ri0m2j"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Predicción del próximo caracter"
      ],
      "metadata": {
        "id": "nGp1fPNLkUX6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El objetivo de este notebook es entrenar un modelo de lenguaje basado en arquitecturas recurrentes a partir de un corpus.\n",
        "\n",
        "La fuente de datos a utilizar es un dataset consistente en transcriptiones de TED Talks en inglés. El mismo puede hallarse el el siguiente link: [Kaggle - TED Talks](https://www.kaggle.com/datasets/miguelcorraljr/ted-ultimate-dataset)\n",
        "\n",
        "En el transcurso del ejercicio se explorarán técnicas de generación de secuencias y se medirá la calidad de las mismas calculando la *perplejidad*.\n",
        "Una vez entrenado el modelo, se lo empleará para generar los siguientes $\\mathit{n}$ caracteres, dada una secuencia de inicio.\n",
        "\n",
        "Se incluye, además, una implementación de Beam Search para generar secuencias sobre el mismo dataset."
      ],
      "metadata": {
        "id": "4F6N_iT1kthx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Librerías"
      ],
      "metadata": {
        "id": "4aPNVaEPuzdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Input, TimeDistributed, CategoryEncoding, SimpleRNN, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "\n",
        "from scipy.special import softmax\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "Q8xGnMVAu4kh"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Carga de datos y preprocesamiento"
      ],
      "metadata": {
        "id": "uwS8--5Mu51j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install gdown\n",
        "!gdown 1D1XSN3V-Oq_OQhYFmaQeu8CgfqY_8O-L"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZe0FmfdKznH",
        "outputId": "093109a6-84f8-4e77-d7e0-a8916f618dea"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1D1XSN3V-Oq_OQhYFmaQeu8CgfqY_8O-L\n",
            "To: /content/ted_talks_en.csv\n",
            "100% 45.5M/45.5M [00:00<00:00, 147MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('ted_talks_en.csv')\n",
        "\n",
        "# Filtrar el dataframe tomando sólo las TED Talks que tienen a \"computers\" entre sus tópicos.\n",
        "# Tomar sólo 30 registros para evitar exceder la memoria disponible en Colab.\n",
        "df = df[df['topics'].apply(lambda x: 'computers' in x)][['transcript']].iloc[:30]\n",
        "print('\\nCantidad de documentos:', df.shape[0])\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "HoEdHDtZxvaj",
        "outputId": "031423ad-d50e-4c3e-a064-d7230b7a2b68"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cantidad de documentos: 30\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            transcript\n",
              "2    (Music: \"The Sound of Silence,\" Simon & Garfun...\n",
              "59   This meeting has really been about a digital r...\n",
              "104  I do two things: I design mobile computers and...\n",
              "155  There's an ancient and universal concept that ...\n",
              "194  A great way to start, I think, with my view of..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6b4a8571-9f36-4aec-ad0f-18758d600cca\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>transcript</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>(Music: \"The Sound of Silence,\" Simon &amp; Garfun...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>This meeting has really been about a digital r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>I do two things: I design mobile computers and...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>155</th>\n",
              "      <td>There's an ancient and universal concept that ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>194</th>\n",
              "      <td>A great way to start, I think, with my view of...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6b4a8571-9f36-4aec-ad0f-18758d600cca')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6b4a8571-9f36-4aec-ad0f-18758d600cca button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6b4a8571-9f36-4aec-ad0f-18758d600cca');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-817e687d-620b-41bf-93bf-1ee325035316\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-817e687d-620b-41bf-93bf-1ee325035316')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-817e687d-620b-41bf-93bf-1ee325035316 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 30,\n  \"fields\": [\n    {\n      \"column\": \"transcript\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 30,\n        \"samples\": [\n          \"It feels like we're all suffering from information overload or data glut. And the good news is there might be an easy solution to that, and that's using our eyes more. So, visualizing information, so that we can see the patterns and connections that matter and then designing that information so it makes more sense, or it tells a story, or allows us to focus only on the information that's important. Failing that, visualized information can just look really cool. So, let's see. This is the $Billion Dollar o-Gram, and this image arose out of frustration I had with the reporting of billion-dollar amounts in the press. That is, they're meaningless without context: 500 billion for this pipeline, 20 billion for this war. It doesn't make any sense, so the only way to understand it is visually and relatively. So I scraped a load of reported figures from various news outlets and then scaled the boxes according to those amounts. And the colors here represent the motivation behind the money. So purple is \\\"fighting,\\\" and red is \\\"giving money away,\\\" and green is \\\"profiteering.\\\" And what you can see straight away is you start to have a different relationship to the numbers. You can literally see them. But more importantly, you start to see patterns and connections between numbers that would otherwise be scattered across multiple news reports. Let me point out some that I really like. This is OPEC's revenue, this green box here \\u2014 780 billion a year. And this little pixel in the corner \\u2014 three billion \\u2014 that's their climate change fund. Americans, incredibly generous people \\u2014 over 300 billion a year, donated to charity every year, compared with the amount of foreign aid given by the top 17 industrialized nations at 120 billion. Then of course, the Iraq War, predicted to cost just 60 billion back in 2003. And it mushroomed slightly. Afghanistan and Iraq mushroomed now to 3,000 billion. So now it's great because now we have this texture, and we can add numbers to it as well. So we could say, well, a new figure comes out ... let's see African debt. How much of this diagram do you think might be taken up by the debt that Africa owes to the West? Let's take a look. So there it is: 227 billion is what Africa owes. And the recent financial crisis, how much of this diagram might that figure take up? What has that cost the world? Let's take a look at that. Dooosh \\u2014 Which I think is the appropriate sound effect for that much money: 11,900 billion. So, by visualizing this information, we turned it into a landscape that you can explore with your eyes, a kind of map really, a sort of information map. And when you're lost in information, an information map is kind of useful. So I want to show you another landscape now. We need to imagine what a landscape of the world's fears might look like. Let's take a look. This is Mountains Out of Molehills, a timeline of global media panic. (Laughter) So, I'll label this for you in a second. But the height here, I want to point out, is the intensity of certain fears as reported in the media. Let me point them out. So this, swine flu \\u2014 pink. Bird flu. SARS \\u2014 brownish here. Remember that one? The millennium bug, terrible disaster. These little green peaks are asteroid collisions. (Laughter) And in summer, here, killer wasps. (Laughter) So these are what our fears look like over time in our media. But what I love \\u2014 and I'm a journalist \\u2014 and what I love is finding hidden patterns; I love being a data detective. And there's a very interesting and odd pattern hidden in this data that you can only see when you visualize it. Let me highlight it for you. See this line, this is a landscape for violent video games. As you can see, there's a kind of odd, regular pattern in the data, twin peaks every year. If we look closer, we see those peaks occur at the same month every year. Why? Well, November, Christmas video games come out, and there may well be an upsurge in the concern about their content. But April isn't a particularly massive month for video games. Why April? Well, in April 1999 was the Columbine shooting, and since then, that fear has been remembered by the media and echoes through the group mind gradually through the year. You have retrospectives, anniversaries, court cases, even copy-cat shootings, all pushing that fear into the agenda. And there's another pattern here as well. Can you spot it? See that gap there? There's a gap, and it affects all the other stories. Why is there a gap there? You see where it starts? September 2001, when we had something very real to be scared about. So, I've been working as a data journalist for about a year, and I keep hearing a phrase all the time, which is this: \\\"Data is the new oil.\\\" Data is the kind of ubiquitous resource that we can shape to provide new innovations and new insights, and it's all around us, and it can be mined very easily. It's not a particularly great metaphor in these times, especially if you live around the Gulf of Mexico, but I would, perhaps, adapt this metaphor slightly, and I would say that data is the new soil. Because for me, it feels like a fertile, creative medium. Over the years, online, we've laid down a huge amount of information and data, and we irrigate it with networks and connectivity, and it's been worked and tilled by unpaid workers and governments. And, all right, I'm kind of milking the metaphor a little bit. But it's a really fertile medium, and it feels like visualizations, infographics, data visualizations, they feel like flowers blooming from this medium. But if you look at it directly, it's just a lot of numbers and disconnected facts. But if you start working with it and playing with it in a certain way, interesting things can appear and different patterns can be revealed. Let me show you this. Can you guess what this data set is? What rises twice a year, once in Easter and then two weeks before Christmas, has a mini peak every Monday, and then flattens out over the summer? I'll take answers. (Audience: Chocolate.) David McCandless: Chocolate. You might want to get some chocolate in. Any other guesses? (Audience: Shopping.) DM: Shopping. Yeah, retail therapy might help. (Audience: Sick leave.) DM: Sick leave. Yeah, you'll definitely want to take some time off. Shall we see? (Laughter) (Applause) So, the information guru Lee Byron and myself, we scraped 10,000 status Facebook updates for the phrase \\\"break-up\\\" and \\\"broken-up\\\" and this is the pattern we found \\u2014 people clearing out for Spring Break, (Laughter) coming out of very bad weekends on a Monday, being single over the summer, and then the lowest day of the year, of course: Christmas Day. Who would do that? So there's a titanic amount of data out there now, unprecedented. But if you ask the right kind of question, or you work it in the right kind of way, interesting things can emerge. So information is beautiful. Data is beautiful. I wonder if I could make my life beautiful. And here's my visual C.V. I'm not quite sure I've succeeded. Pretty blocky, the colors aren't that great. But I wanted to convey something to you. I started as a programmer, and then I worked as a writer for many years, about 20 years, in print, online and then in advertising, and only recently have I started designing. And I've never been to design school. I've never studied art or anything. I just kind of learned through doing. And when I started designing, I discovered an odd thing about myself. I already knew how to design, but it wasn't like I was amazingly brilliant at it, but more like I was sensitive to the ideas of grids and space and alignment and typography. It's almost like being exposed to all this media over the years had instilled a kind of dormant design literacy in me. And I don't feel like I'm unique. I feel that everyday, all of us now are being blasted by information design. It's being poured into our eyes through the Web, and we're all visualizers now; we're all demanding a visual aspect to our information. There's something almost quite magical about visual information. It's effortless, it literally pours in. And if you're navigating a dense information jungle, coming across a beautiful graphic or a lovely data visualization, it's a relief, it's like coming across a clearing in the jungle. I was curious about this, so it led me to the work of a Danish physicist called Tor Norretranders, and he converted the bandwidth of the senses into computer terms. So here we go. This is your senses, pouring into your senses every second. Your sense of sight is the fastest. It has the same bandwidth as a computer network. Then you have touch, which is about the speed of a USB key. And then you have hearing and smell, which has the throughput of a hard disk. And then you have poor old taste, which is like barely the throughput of a pocket calculator. And that little square in the corner, a naught .7 percent, that's the amount we're actually aware of. So a lot of your vision \\u2014 the bulk of it is visual, and it's pouring in. It's unconscious. The eye is exquisitely sensitive to patterns in variations in color, shape and pattern. It loves them, and it calls them beautiful. It's the language of the eye. If you combine the language of the eye with the language of the mind, which is about words and numbers and concepts, you start speaking two languages simultaneously, each enhancing the other. So, you have the eye, and then you drop in the concepts. And that whole thing \\u2014 it's two languages both working at the same time. So we can use this new kind of language, if you like, to alter our perspective or change our views. Let me ask you a simple question with a really simple answer: Who has the biggest military budget? It's got to be America, right? Massive. 609 billion in 2008 \\u2014 607, rather. So massive, in fact, that it can contain all the other military budgets in the world inside itself. Gobble, gobble, gobble, gobble, gobble. Now, you can see Africa's total debt there and the U.K. budget deficit for reference. So that might well chime with your view that America is a sort of warmongering military machine, out to overpower the world with its huge industrial-military complex. But is it true that America has the biggest military budget? Because America is an incredibly rich country. In fact, it's so massively rich that it can contain the four other top industrialized nations' economies inside itself, it's so vastly rich. So its military budget is bound to be enormous. So, to be fair and to alter our perspective, we have to bring in another data set, and that data set is GDP, or the country's earnings. Who has the biggest budget as a proportion of GDP? Let's have a look. That changes the picture considerably. Other countries pop into view that you, perhaps, weren't considering, and American drops into eighth. Now you can also do this with soldiers. Who has the most soldiers? It's got to be China. Of course, 2.1 million. Again, chiming with your view that China has a militarized regime ready to, you know, mobilize its enormous forces. But of course, China has an enormous population. So if we do the same, we see a radically different picture. China drops to 124th. It actually has a tiny army when you take other data into consideration. So, absolute figures, like the military budget, in a connected world, don't give you the whole picture. They're not as true as they could be. We need relative figures that are connected to other data so that we can see a fuller picture, and then that can lead to us changing our perspective. As Hans Rosling, the master, my master, said, \\\"Let the dataset change your mindset.\\\" And if it can do that, maybe it can also change your behavior. Take a look at this one. I'm a bit of a health nut. I love taking supplements and being fit, but I can never understand what's going on in terms of evidence. There's always conflicting evidence. Should I take vitamin C? Should I be taking wheatgrass? This is a visualization of all the evidence for nutritional supplements. This kind of diagram is called a balloon race. So the higher up the image, the more evidence there is for each supplement. And the bubbles correspond to popularity as regards to Google hits. So you can immediately apprehend the relationship between efficacy and popularity, but you can also, if you grade the evidence, do a \\\"worth it\\\" line. So supplements above this line are worth investigating, but only for the conditions listed below, and then the supplements below the line are perhaps not worth investigating. Now this image constitutes a huge amount of work. We scraped like 1,000 studies from PubMed, the biomedical database, and we compiled them and graded them all. And it was incredibly frustrating for me because I had a book of 250 visualizations to do for my book, and I spent a month doing this, and I only filled two pages. But what it points to is that visualizing information like this is a form of knowledge compression. It's a way of squeezing an enormous amount of information and understanding into a small space. And once you've curated that data, and once you've cleaned that data, and once it's there, you can do cool stuff like this. So I converted this into an interactive app, so I can now generate this application online \\u2014 this is the visualization online \\u2014 and I can say, \\\"Yeah, brilliant.\\\" So it spawns itself. And then I can say, \\\"Well, just show me the stuff that affects heart health.\\\" So let's filter that out. So heart is filtered out, so I can see if I'm curious about that. I think, \\\"No, no. I don't want to take any synthetics, I just want to see plants and \\u2014 just show me herbs and plants. I've got all the natural ingredients.\\\" Now this app is spawning itself from the data. The data is all stored in a Google Doc, and it's literally generating itself from that data. So the data is now alive; this is a living image, and I can update it in a second. New evidence comes out. I just change a row on a spreadsheet. Doosh! Again, the image recreates itself. So it's cool. It's kind of living. But it can go beyond data, and it can go beyond numbers. I like to apply information visualization to ideas and concepts. This is a visualization of the political spectrum, an attempt for me to try and understand how it works and how the ideas percolate down from government into society and culture, into families, into individuals, into their beliefs and back around again in a cycle. What I love about this image is it's made up of concepts, it explores our worldviews and it helps us \\u2014 it helps me anyway \\u2014 to see what others think, to see where they're coming from. And it feels just incredibly cool to do that. What was most exciting for me designing this was that, when I was designing this image, I desperately wanted this side, the left side, to be better than the right side \\u2014 being a journalist, a Left-leaning person \\u2014 but I couldn't, because I would have created a lopsided, biased diagram. So, in order to really create a full image, I had to honor the perspectives on the right-hand side and at the same time, uncomfortably recognize how many of those qualities were actually in me, which was very, very annoying and uncomfortable. (Laughter) But not too uncomfortable, because there's something unthreatening about seeing a political perspective, versus being told or forced to listen to one. You're capable of holding conflicting viewpoints joyously when you can see them. It's even fun to engage with them because it's visual. So that's what's exciting to me, seeing how data can change my perspective and change my mind midstream \\u2014 beautiful, lovely data. So, just to wrap up, I wanted to say that it feels to me that design is about solving problems and providing elegant solutions, and information design is about solving information problems. It feels like we have a lot of information problems in our society at the moment, from the overload and the saturation to the breakdown of trust and reliability and runaway skepticism and lack of transparency, or even just interestingness. I mean, I find information just too interesting. It has a magnetic quality that draws me in. So, visualizing information can give us a very quick solution to those kinds of problems. Even when the information is terrible, the visual can be quite beautiful. Often we can get clarity or the answer to a simple question very quickly, like this one, the recent Icelandic volcano. Which was emitting the most CO2? Was it the planes or the volcano, the grounded planes or the volcano? So we can have a look. We look at the data and we see: Yep, the volcano emitted 150,000 tons; the grounded planes would have emitted 345,000 if they were in the sky. So essentially, we had our first carbon-neutral volcano. (Laughter) (Applause) And that is beautiful. Thank you. (Applause)\",\n          \"Information technology grows in an exponential manner. It's not linear. And our intuition is linear. When we walked through the savanna a thousand years ago we made linear predictions where that animal would be, and that worked fine. It's hardwired in our brains. But the pace of exponential growth is really what describes information technologies. And it's not just computation. There is a big difference between linear and exponential growth. If I take 30 steps linearly \\u2014 one, two, three, four, five \\u2014 I get to 30. If I take 30 steps exponentially \\u2014 two, four, eight, 16 \\u2014 I get to a billion. It makes a huge difference. And that really describes information technology. When I was a student at MIT, we all shared one computer that took up a whole building. The computer in your cellphone today is a million times cheaper, a million times smaller, a thousand times more powerful. That's a billion-fold increase in capability per dollar that we've actually experienced since I was a student. And we're going to do it again in the next 25 years. Information technology progresses through a series of S-curves where each one is a different paradigm. So people say, \\\"What's going to happen when Moore's Law comes to an end?\\\" Which will happen around 2020. We'll then go to the next paradigm. And Moore's Law was not the first paradigm to bring exponential growth to computing. The exponential growth of computing started decades before Gordon Moore was even born. And it doesn't just apply to computation. It's really any technology where we can measure the underlying information properties. Here we have 49 famous computers. I put them in a logarithmic graph. The logarithmic scale hides the scale of the increase, because this represents trillions-fold increase since the 1890 census. In 1950s they were shrinking vacuum tubes, making them smaller and smaller. They finally hit a wall; they couldn't shrink the vacuum tube any more and keep the vacuum. And that was the end of the shrinking of vacuum tubes, but it was not the end of the exponential growth of computing. We went to the fourth paradigm, transistors, and finally integrated circuits. When that comes to an end we'll go to the sixth paradigm; three-dimensional self-organizing molecular circuits. But what's even more amazing, really, than this fantastic scale of progress, is that \\u2014 look at how predictable this is. I mean this went through thick and thin, through war and peace, through boom times and recessions. The Great Depression made not a dent in this exponential progression. We'll see the same thing in the economic recession we're having now. At least the exponential growth of information technology capability will continue unabated. And I just updated these graphs. Because I had them through 2002 in my book, \\\"The Singularity is Near.\\\" So we updated them, so I could present it here, to 2007. And I was asked, \\\"Well aren't you nervous? Maybe it kind of didn't stay on this exponential progression.\\\" I was a little nervous because maybe the data wouldn't be right, but I've done this now for 30 years, and it has stayed on this exponential progression. Look at this graph here.You could buy one transistor for a dollar in 1968. You can buy half a billion today, and they are actually better, because they are faster. But look at how predictable this is. And I'd say this knowledge is over-fitting to past data. I've been making these forward-looking predictions for about 30 years. And the cost of a transistor cycle, which is a measure of the price performance of electronics, comes down about every year. That's a 50 percent deflation rate. And it's also true of other examples, like DNA data or brain data. But we more than make up for that. We actually ship more than twice as much of every form of information technology. We've had 18 percent growth in constant dollars in every form of information technology for the last half-century, despite the fact that you can get twice as much of it each year. This is a completely different example. This is not Moore's Law. The amount of DNA data we've sequenced has doubled every year. The cost has come down by half every year. And this has been a smooth progression since the beginning of the genome project. And halfway through the project, skeptics said, \\\"Well, this is not working out. You're halfway through the genome project and you've finished one percent of the project.\\\" But that was really right on schedule. Because if you double one percent seven more times, which is exactly what happened, you get 100 percent. And the project was finished on time. Communication technologies: 50 different ways to measure this, the number of bits being moved around, the size of the Internet. But this has progressed at an exponential pace. This is deeply democratizing. I wrote, over 20 years ago in \\\"The Age of Intelligent Machines,\\\" when the Soviet Union was going strong, that it would be swept away by this growth of decentralized communication. And we will have plenty of computation as we go through the 21st century to do things like simulate regions of the human brain. But where will we get the software? Some critics say, \\\"Oh, well software is stuck in the mud.\\\" But we are learning more and more about the human brain. Spatial resolution of brain scanning is doubling every year. The amount of data we're getting about the brain is doubling every year. And we're showing that we can actually turn this data into working models and simulations of brain regions. There is about 20 regions of the brain that have been modeled, simulated and tested: the auditory cortex, regions of the visual cortex; cerebellum, where we do our skill formation; slices of the cerebral cortex, where we do our rational thinking. And all of this has fueled an increase, very smooth and predictable, of productivity. We've gone from 30 dollars to 130 dollars in constant dollars in the value of an average hour of human labor, fueled by this information technology. And we're all concerned about energy and the environment. Well this is a logarithmic graph. This represents a smooth doubling, every two years, of the amount of solar energy we're creating, particularly as we're now applying nanotechnology, a form of information technology, to solar panels. And we're only eight doublings away from it meeting 100 percent of our energy needs. And there is 10 thousand times more sunlight than we need. We ultimately will merge with this technology. It's already very close to us. When I was a student it was across campus, now it's in our pockets. What used to take up a building now fits in our pockets. What now fits in our pockets would fit in a blood cell in 25 years. And we will begin to actually deeply influence our health and our intelligence, as we get closer and closer to this technology. Based on that we are announcing, here at TED, in true TED tradition, Singularity University. It's a new university that's founded by Peter Diamandis, who is here in the audience, and myself. It's backed by NASA and Google, and other leaders in the high-tech and science community. And our goal was to assemble the leaders, both teachers and students, in these exponentially growing information technologies, and their application. But Larry Page made an impassioned speech at our organizing meeting, saying we should devote this study to actually addressing some of the major challenges facing humanity. And if we did that, then Google would back this. And so that's what we've done. The last third of the nine-week intensive summer session will be devoted to a group project to address some major challenge of humanity. Like for example, applying the Internet, which is now ubiquitous, in the rural areas of China or in Africa, to bringing health information to developing areas of the world. And these projects will continue past these sessions, using collaborative interactive communication. All the intellectual property that is created and taught will be online and available, and developed online in a collaborative fashion. Here is our founding meeting. But this is being announced today. It will be permanently headquartered in Silicon Valley, at the NASA Ames research center. There are different programs for graduate students, for executives at different companies. The first six tracks here \\u2014 artificial intelligence, advanced computing technologies, biotechnology, nanotechnology \\u2014 are the different core areas of information technology. Then we are going to apply them to the other areas, like energy, ecology, policy law and ethics, entrepreneurship, so that people can bring these new technologies to the world. So we're very appreciative of the support we've gotten from both the intellectual leaders, the high-tech leaders, particularly Google and NASA. This is an exciting new venture. And we invite you to participate. Thank you very much. (Applause)\",\n          \"We're 25, 26 years after the advent of the Macintosh, which was an astoundingly seminal event in the history of human-machine interface and in computation in general. It fundamentally changed the way that people thought about computation, thought about computers, how they used them and who and how many people were able to use them. It was such a radical change, in fact, that the early Macintosh development team in '82, '83, '84 had to write an entirely new operating system from the ground up. Now, this is an interesting little message, and it's a lesson that has since, I think, been forgotten or lost or something, and that is, namely, that the OS is the interface. The interface is the OS. It's like the land and the king (i.e. Arthur) they're inseparable, they are one. And to write a new operating system was not a capricious matter. It wasn't just a matter of tuning up some graphics routines. There were no graphics routines. There were no mouse drivers. So it was a necessity. But in the quarter-century since then, we've seen all of the fundamental supporting technologies go berserk. So memory capacity and disk capacity have been multiplied by something between 10,000 and a million. Same thing for processor speeds. Networks, we didn't have networks at all at the time of the Macintosh's introduction, and that has become the single most salient aspect of how we live with computers. And, of course, graphics: Today 84 dollars and 97 cents at Best Buy buys you more graphics power than you could have gotten for a million bucks from SGI only a decade ago. So we've got that incredible ramp-up. Then, on the side, we've got the Web and, increasingly, the cloud, which is fantastic, but also \\u2014 in the regard in which an interface is fundamental \\u2014 kind of a distraction. So we've forgotten to invent new interfaces. Certainly we've seen in recent years a lot of change in that regard, and people are starting to wake up about that. So what happens next? Where do we go from there? The problem, as we see it, has to do with a single, simple word: \\\"space,\\\" or a single, simple phrase: \\\"real world geometry.\\\" Computers and the programming languages that we talk to them in, that we teach them in, are hideously insensate when it comes to space. They don't understand real world space. It's a funny thing because the rest of us occupy it quite frequently and quite well. They also don't understand time, but that's a matter for a separate talk. So what happens if you start to explain space to them? One thing you might get is something like the Luminous Room. The Luminous Room is a system in which it's considered that input and output spaces are co-located. That's a strangely simple, and yet unexplored idea, right? When you use a mouse, your hand is down here on the mouse pad. It's not even on the same plane as what you're talking about: The pixels are up on the display. So here was a room in which all the walls, floors, ceilings, pets, potted plants, whatever was in there, were capable, not only of display but of sensing as well. And that means input and output are in the same space enabling stuff like this. That's a digital storage in a physical container. The contract is the same as with real word objects in real world containers. Has to come back out, whatever you put in. This little design experiment that was a small office here knew a few other tricks as well. If you presented it with a chess board, it tried to figure out what you might mean by that. And if there was nothing for them to do, the chess pieces eventually got bored and hopped away. The academics who were overseeing this work thought that that was too frivolous, so we built deadly serious applications like this optics prototyping workbench in which a toothpaste cap on a cardboard box becomes a laser. The beam splitters and lenses are represented by physical objects, and the system projects down the laser beam path. So you've got an interface that has no interface. You operate the world as you operate the real world, which is to say, with your hands. Similarly, a digital wind tunnel with digital wind flowing from right to left \\u2014 not that remarkable in a sense; we didn't invent the mathematics. But if you displayed that on a CRT or flat panel display, it would be meaningless to hold up an arbitrary object, a real world object in that. Here, the real world merges with the simulation. And finally, to pull out all the stops, this is a system called Urp, for urban planners, in which we give architects and urban planners back the models that we confiscated when we insisted that they use CAD systems. And we make the machine meet them half way. It projects down digital shadows, as you see here. And if you introduce tools like this inverse clock, then you can control the sun's position in the sky. That's 8 a.m. shadows. They get a little shorter at 9 a.m. There you are, swinging the sun around. Short shadows at noon and so forth. And we built up a series of tools like this. There are inter-shadowing studies that children can operate, even though they don't know anything about urban planning: To move a building, you simply reach out your hand and you move the building. A material wand makes the building into a sort of Frank Gehry thing that reflects light in all directions. Are you blinding passers by and motorists on the freeways? A zoning tool connects distant structures, a building and a roadway. Are you going to get sued by the zoning commission? And so forth. Now, if these ideas seem familiar or perhaps even a little dated, that's great; they should seem familiar. This work is 15 years old. This stuff was undertaken at MIT and the Media Lab under the incredible direction of Professor Hiroshi Ishii, director of the Tangible Media Group. But it was that work that was seen by Alex McDowell, one of the world's legendary production designers. But Alex was preparing a little, sort of obscure, indie, arthouse film called \\\"Minority Report\\\" for Steven Spielberg, and invited us to come out from MIT and design the interfaces that would appear in that film. And the great thing about it was that Alex was so dedicated to the idea of verisimilitude, the idea that the putative 2054 that we were painting in the film be believable, that he allowed us to take on that design work as if it were an R&D effort. And the result is sort of gratifyingly perpetual. People still reference those sequences in \\\"Minority Report\\\" when they talk about new UI design. So this led full circle, in a strange way, to build these ideas into what we believe is the necessary future of human machine interface: the Spatial Operating Environment, we call it. So here we have a bunch of stuff, some images. And, using a hand, we can actually exercise six degrees of freedom, six degrees of navigational control. And it's fun to fly through Mr. Beckett's eye. And you can come back out through the scary orangutan. And that's all well and good. Let's do something a little more difficult. Here, we have a whole bunch of disparate images. We can fly around them. So navigation is a fundamental issue. You have to be able to navigate in 3D. Much of what we want computers to help us with in the first place is inherently spatial. And the part that isn't spatial can often be spatialized to allow our wetware to make greater sense of it. Now we can distribute this stuff in many different ways. So we can throw it out like that. Let's reset it. We can organize it this way. And, of course, it's not just about navigation, but about manipulation as well. So if we don't like stuff, or we're intensely curious about Ernst Haeckel's scientific falsifications, we can pull them out like that. And then if it's time for analysis, we can pull back a little bit and ask for a different distribution. Let's just come down a bit and fly around. So that's a different way to look at stuff. If you're of a more analytical nature then you might want, actually, to look at this as a color histogram. So now we've got the stuff color-sorted, angle maps onto color. And now, if we want to select stuff, 3D, space, the idea that we're tracking hands in real space becomes really important because we can reach in, not in 2D, not in fake 2D, but in actual 3D. Here are some selection planes. And we'll perform this Boolean operation because we really love yellow and tapirs on green grass. So, from there to the world of real work. Here's a logistics system, a small piece of one that we're currently building. There're a lot of elements. And one thing that's very important is to combine traditional tabular data with three-dimensional and geospatial information. So here's a familiar place. And we'll bring this back here for a second. Maybe select a little bit of that. And bring out this graph. And we should, now, be able to fly in here and have a closer look. These are logistics elements that are scattered across the United States. One thing that three-dimensional interactions and the general idea of imbuing computation with space affords you is a final destruction of that unfortunate one-to-one pairing between human beings and computers. That's the old way, that's the old mantra: one machine, one human, one mouse, one screen. Well, that doesn't really cut it anymore. In the real world, we have people who collaborate; we have people who have to work together, and we have many different displays. And we might want to look at these various images. We might want to ask for some help. The author of this new pointing device is sitting over there, so I can pull this from there to there. These are unrelated machines, right? So the computation is space soluble and network soluble. So I'm going to leave that over there because I have a question for Paul. Paul is the designer of this wand, and maybe its easiest for him to come over here and tell me in person what's going on. So let me get some of these out of the way. Let's pull this apart: I'll go ahead and explode it. Kevin, can you help? Let me see if I can help us find the circuit board. Mind you, it's a sort of gratuitous field-stripping exercise, but we do it in the lab all the time. All right. So collaborative work, whether it's immediately co-located or distant and distinct, is always important. And again, that stuff needs to be undertaken in the context of space. And finally, I'd like to leave you with a glimpse that takes us back to the world of imagery. This is a system called TAMPER, which is a slightly whimsical look at what the future of editing and media manipulation systems might be. We at Oblong believe that media should be accessible in much more fine-grained form. So we have a large number of movies stuck inside here. And let's just pick out a few elements. We can zip through them as a possibility. We can grab elements off the front, where upon they reanimate, come to life, and drag them down onto the table here. We'll go over to Jacques Tati here and grab our blue friend and put him down on the table as well. We may need more than one. And we probably need, well, we probably need a cowboy to be quite honest. (Laughter) Yeah, let's take that one. (Laughter) You see, cowboys and French farce people don't go well together, and the system knows that. Let me leave with one final thought, and that is that one of the greatest English language writers of the last three decades suggested that great art is always a gift. And he wasn't talking about whether the novel costs 24.95 [dollars], or whether you have to spring 70 million bucks to buy the stolen Vermeer; he was talking about the circumstances of its creation and of its existence. And I think that it's time that we asked for the same from technology. Technology is capable of expressing and being imbued with a certain generosity, and we need to demand that, in fact. For some of this kind of technology, ground center is a combination of design, which is crucially important. We can't have advances in technology any longer unless design is integrated from the very start. And, as well, as of efficacy, agency. We're, as human beings, the creatures that create, and we should make sure that our machines aid us in that task and are built in that same image. So I will leave you with that. Thank you. (Applause) Chris Anderson: So to ask the obvious question \\u2014 actually this is from Bill Gates \\u2014 when? (John Underkoffler: When?) CA: When real? When for us, not just in a lab and on a stage? Can it be for every man, or is this just for corporations and movie producers? JU: No, it has to be for every human being. That's our goal entirely. We won't have succeeded unless we take that next big step. I mean it's been 25 years. Can there really be only one interface? There can't. CA: But does that mean that, at your desk or in your home, you need projectors, cameras? You know, how can it work? JU: No, this stuff will be built into the bezel of every display. It'll be built into architecture. The gloves go away in a matter of months or years. So this is the inevitability about it. CA: So, in your mind, five years time, someone can buy this as part of a standard computer interface? JU: I think in five years time when you buy a computer, you'll get this. CA: Well that's cool. (Applause) The world has a habit of surprising us as to how these things are actually used. What do you think, what in your mind is the first killer app for this? JU: That's a good question, and we ask ourselves that every day. At the moment, our early-adopter customers \\u2014 and these systems are deployed out in the real world \\u2014 do all the big data intensive, data heavy problems with it. So, whether it's logistics and supply chain management or natural gas and resource extraction, financial services, pharmaceuticals, bioinformatics, those are the topics right now, but that's not a killer app. And I understand what you're asking. CA: C'mon, c'mon. Martial arts, games. C'mon. (Laughter) John, thank you for making science-fiction real. JU: It's been a great pleasure. Thank you to you all. (Applause)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar cada oración en una lista.\n",
        "text = list(df.loc[:, 'transcript'])\n",
        "text[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "60x0bF51vGOy",
        "outputId": "4d7285fb-672d-4ef5-9749-d126ede59d2c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'(Music: \"The Sound of Silence,\" Simon & Garfunkel) Hello voice mail, my old friend. (Laughter) I\\'ve called for tech support again. I ignored my boss\\'s warning. I called on a Monday morning. Now it\\'s evening, and my dinner first grew cold, and then grew mold. I\\'m still on hold. I\\'m listening to the sounds of silence. I don\\'t think you understand. I think your phone lines are unmanned. I punched every touch tone I was told, but I\\'ve still spent 18 hours on hold. It\\'s not enough your software crashed my Mac, and it constantly hangs and bombs — it erased my ROMs! Now the Mac makes the sounds of silence. In my dreams I fantasize of wreaking vengeance on you guys. Say your motorcycle crashes. Blood comes gushing from your gashes. With your fading strength, you call 9-1-1 and you pray for a trained MD. But you get me. (Laughter) And you listen to the sounds of silence. (Music) (Applause) Thank you. Good evening and welcome to: \"Spot the TED Presenter Who Used to Be a Broadway Accompanist.\" (Laughter) When I was offered the Times column six years ago, the deal was like this: you\\'ll be sent the coolest, hottest, slickest new gadgets. Every week, it\\'ll arrive at your door. You get to try them out, play with them, evaluate them until the novelty wears out, before you have to send them back, and you\\'ll get paid for it. You can think about it, if you want. So, I\\'ve always been a technology nut, and I absolutely love it. The job, though, came with one small downside, and that is, they intended to publish my email address at the end of every column. And what I\\'ve noticed is — first of all, you get an incredible amount of email. If you ever are feeling lonely, get a New York Times column, because you will get hundreds and hundreds and hundreds of emails. And the email I\\'m getting a lot today is about frustration. People are feeling like things — Ok, I just had an alarm come up on my screen. Lucky you can\\'t see it. People are feeling overwhelmed. They\\'re feeling like it\\'s too much technology, too fast. It may be good technology, but I feel like there\\'s not enough of a support structure. There\\'s not enough help. There\\'s not enough thought put into the design of it to make it easy and enjoyable to use. One time I wrote a column about my efforts to reach Dell Technical Support, and within 12 hours, there were 700 messages from readers on the feedback boards on the Times website, from users saying, \"\"Me too, and here\\'s my tale of woe.\" I call it \"software rage.\" And man, let me tell you, whoever figures out how to make money off of this frustration will — Oh, how did that get up there? Just kidding. (Laughter) Ok, so why is the problem accelerating? And part of the problem is, ironically, because the industry has put so much thought into making things easier to use. I\\'ll show you what I mean. This is what the computer interface used to look like, DOS. Over the years, it\\'s gotten easier to use. This is the original Mac operating system. Reagan was President. Madonna was still a brunette. And the entire operating system — this is the good part — the entire operating system fit in 211 k. You couldn\\'t put the Mac OS X logo in 211 k! (Laughter) So the irony is, that as these things became easier to use, a less technical, broader audience was coming into contact with this equipment for the first time. I once had the distinct privilege of sitting in on the Apple call center for a day. The guy had a duplicate headset for me to listen to. And the calls that — you know how they say, \"Your call may be recorded for quality assurance?\" Uh-uh. Your call may be recorded so that they can collect the funniest dumb user stories and pass them around on a CD. (Laughter) Which they do. (Laughter) And I have a copy. (Laughter) It\\'s in your gift bag. No, no. With your voices on it! So, some of the stories are just so classic, and yet so understandable. A woman called Apple to complain that her mouse was squeaking. Making a squeaking noise. And the technician said, \"Well, ma\\'am, what do you mean your mouse is squeaking?\" She says, \"All I can tell you is that it squeaks louder, the faster I move it across the screen.\" (Laughter) And the technician\\'s like, \"Ma\\'am, you\\'ve got the mouse up against the screen?\" She goes, \"Well, the message said, \\'Click here to continue.\\'\" (Laughter) Well, if you like that one — how much time have we got? Another one, a guy called — this is absolutely true — his computer had crashed, and he told the technician he couldn\\'t restart it, no matter how many times he typed \"11.\" And the technician said, \"What? Why are you typing 11?\" He said, \"The message says, \\'Error Type 11.\\'\" (Laughter) So, we must admit that some of the blame falls squarely at the feet of the users. But why is the technical overload crisis, the complexity crisis, accelerating now? In the hardware world, it\\'s because we the consumers want everything to be smaller, smaller, smaller. So the gadgets are getting tinier and tinier, but our fingers are essentially staying the same size. So it gets to be more and more of a challenge. Software is subject to another primal force: the mandate to release more and more versions. When you buy a piece of software, it\\'s not like buying a vase or a candy bar, where you own it. It\\'s more like joining a club, where you pay dues every year, and every year, they say, \"We\\'ve added more features, and we\\'ll sell it to you for $99.\" I know one guy who\\'s spent $4,000 just on Photoshop over the years. And software companies make 35 percent of their revenue from just these software upgrades. I call it the Software Upgrade Paradox — which is that if you improve a piece of software enough times, you eventually ruin it. I mean, Microsoft Word was last just a word processor in, you know, the Eisenhower administration. (Laughter) But what\\'s the alternative? Microsoft actually did this experiment. They said, \"Well, wait a minute. Everyone complains that we\\'re adding so many features. Let\\'s create a word processor that\\'s just a word processor: Simple, pure; does not do web pages, is not a database.\" And it came out, and it was called Microsoft Write. And none of you are nodding in acknowledgment, because it died. It tanked. No one ever bought it. I call this the Sport Utility Principle. People like to surround themselves with unnecessary power, right? They don\\'t need the database and the website, but they\\'re like, \"Well, I\\'ll upgrade, because, I might, you know, I might need that someday.\" So the problem is: as you add more features, where are they going to go? Where are you going to stick them? You only have so many design tools. You can do buttons, you can do sliders, pop-up menus, sub-menus. But if you\\'re not careful about how you choose, you wind up with this. (Laughter) This is an un-retouched — this is not a joke — un-retouched photo of Microsoft Word, the copy that you have, with all the toolbars open. You\\'ve obviously never opened all the toolbars, but all you have to type in is this little, teeny window down here. (Laughter) And we\\'ve arrived at the age of interface matrices, where there are so many features and options, you have to do two dimensions, you know: a vertical and a horizontal. You guys all complain about how Microsoft Word is always bulleting your lists and underlining your links automatically. The off switch is in there somewhere. I\\'m telling you — it\\'s there. Part of the art of designing a simple, good interface, is knowing when to use which one of these features. So, here is the log-off dialogue box for Windows 2000. There are only four choices, so why are they in a pop-up menu? It\\'s not like the rest of the screen is so full of other components that you need to collapse the choices. They could have put them all out in view. Here\\'s Apple\\'s take on the exact same dialogue box. (Applause) Thank you — yes, I designed the dialogue box. No, no. Already, we can see that Apple and Microsoft have a severely divergent approach to software design. Microsoft\\'s approach to simplicity tends to be: let\\'s break it down; let\\'s just make it more steps. There are these \"wizards\" everywhere. And you know, there\\'s a new version of Windows coming out this fall. If they continue at this pace, there\\'s absolutely no telling where they might wind up. [Welcome to the Type a Word Wizard] (Laughter) (Applause) \"Welcome to the Type a Word Wizard.\" Ok, I\\'ll bite. Let\\'s click \"Next\" to continue. (Laughter) (Applause) From the drop-down menu, choose the first letter you want to type. Ok. (Laughter) So there is a limit that we don\\'t want to cross. So what is the answer? How do you pack in all these features in a simple, intelligent way? I believe in consistency, when possible, real-world equivalents, trash can folder, when possible, label things, mostly. But I beg of the designers here to break all those rules if they violate the biggest rule of all, which is intelligence. Now what do I mean by that? I\\'m going to give you some examples where intelligence makes something not consistent, but it\\'s better. If you are buying something on the web, you\\'re supposed to put in your address, and you\\'re supposed to choose what country you\\'re from, ok? There are 200 countries in the world. We like to think of the Internet as a global village. I\\'m sorry; it\\'s not one yet. It\\'s mainly like, the United States, Europe, and Japan. So why is \"United States\" in the \"U\"s? (Laughter) You have to scroll, like, seven screensful to get to it. Now, it would be inconsistent to put \"United States\" first, but it would be intelligent. This one\\'s been touched on before, but why in God\\'s name do you shut down a Windows PC by clicking a button called \"Start?\" (Laughter) Here\\'s another pet one of mine: you have a printer. Most of the time, you want to print one copy of your document, in page order, on that printer. So why in God\\'s name do you see this every time you print? It\\'s like a 747 shuttle cockpit. (Laughter) And one of the buttons at the bottom, you\\'ll notice, is not \"Print.\" (Laughter) (Applause) Now, I\\'m not saying that Apple is the only company who has embraced the cult of simplicity. Palm is also, especially in the old days, wonderful about this. I actually got to speak to Palm when they were flying high in the \\'90s, and after the talk, I met one of the employees. He says, \"Nice talk.\" And I said, \"Thank you. What do you do here?\" He said, \"I\\'m a tap counter.\" I\\'m like, \"You\\'re a what?\" He goes, \"Well Jeff Hawkins, the CEO, says, \\'If any task on the Palm Pilot takes more than three taps of the stylus, it\\'s too long, and it has to be redesigned.\\' So I\\'m the tap counter.\" So, I\\'m going to show you an example of a company that does not have a tap counter. (Laughter) This is Microsoft Word. Ok, when you want to create a new blank document in Word — it could happen. (Laughter) You go up to the \"File\" menu and you choose \"New.\" Now, what happens when you choose \"New?\" Do you get a new blank document? You do not. On the opposite side of the monitor, a task bar appears, and somewhere in those links — by the way, not at the top — somewhere in those links is a button that makes you a new document. Ok, so that is a company not counting taps. You know, I don\\'t want to just stand here and make fun of Microsoft ... Yes, I do. (Laughter) (Applause) The Bill Gates song! (Piano music) I\\'ve been a geek forever and I wrote the very first DOS. I put my software and IBM together; I got profit and they got the loss. (Laughter) I write the code that makes the whole world run. I\\'m getting royalties from everyone. Sometimes it\\'s garbage, but the press is snowed. You buy the box; I\\'ll sell the code. Every software company is doing Microsoft\\'s R&D. You can\\'t keep a good idea down these days. Even Windows is a hack. We\\'re kind of based loosely on the Mac. So it\\'s big, so it\\'s slow. You\\'ve got nowhere to go. I\\'m not doing this for praise. I write the code that fits the world today. Big mediocrity in every way. We\\'ve entered planet domination mode. You\\'ll have no choice; you\\'ll buy my code. I am Bill Gates and I write the code. (Applause) But actually, I believe there are really two Microsofts. There\\'s the old one, responsible for Windows and Office. They\\'re dying to throw the whole thing out and start fresh, but they can\\'t. They\\'re locked in, because so many add-ons and other company stuff locks into the old 1982 chassis. But there\\'s also a new Microsoft, that\\'s really doing good, simple interface designs. I liked the Media Center PC. I liked the Microsoft SPOT Watch. The Wireless Watch flopped miserably in the market, but it wasn\\'t because it wasn\\'t simply and beautifully designed. But let\\'s put it this way: would you pay $10 a month to have a watch that has to be recharged every night like your cell phone, and stops working when you leave your area code? (Laughter) So, the signs might indicate that the complexity crunch is only going to get worse. So is there any hope? The screens are getting smaller, people are illuminating, putting manuals in the boxes, things are coming out at a faster pace. It\\'s funny — when Steve Jobs came back to Apple in 1997, after 12 years away, it was the MacWorld Expo — he came to the stage in that black turtleneck and jeans, and he sort of did this. The crowd went wild, but I had just seen — I\\'m like, where have I seen this before? I had just seen the movie \"Evita\" — (Laughter) with Madonna, and I\\'m like, you know what? I\\'ve got to do one about Steve Jobs. (Music) It won\\'t be easy. You\\'ll think I\\'m strange. (Laughter) When I try to explain why I\\'m back, after telling the press Apple\\'s future is black. You won\\'t believe me. All that you see is a kid in his teens who started out in a garage with only a buddy named Woz. (Laughter) You try rhyming with garage! (Laughter) Don\\'t cry for me, Cupertino. (Laughter) The truth is, I never left you. I know the ropes now, know what the tricks are. I made a fortune over at Pixar. (Laughter) Don\\'t cry for me, Cupertino. I\\'ve still got the drive and vision. I still wear sandals in any weather. It\\'s just that these days, they\\'re Gucci leather. (Laughter) (Applause) Thank you. So Steve Jobs had always believed in simplicity and elegance and beauty. And the truth is, for years I was a little depressed, because Americans obviously did not value it, because the Mac had three percent market share, Windows had 95 percent market share — people did not think it was worth putting a price on it. So I was a little depressed. And then I heard Al Gore\\'s talk, and I realized I didn\\'t know the meaning of depressed. (Laughter) But it turns out I was wrong, right? Because the iPod came out, and it violated every bit of common wisdom. Other products cost less; other products had more features, they had voice recorders and FM transmitters. The other products were backed by Microsoft, with an open standard, not Apple\\'s propriety standard. But the iPod won — this is the one they wanted. The lesson was: simplicity sells. And there are signs that the industry is getting the message. This is a little company that\\'s done very well with simplicity and elegance. The Sonos thing — it\\'s catching on. I\\'ve got just a couple examples. Physically, a really cool, elegant thinking coming along lately. When you have a digital camera, how do you get the pictures back to your computer? Well, you either haul around a USB cable, or you buy a card reader and haul that around. Either one, you\\'re going to lose. What I do is, I take out the memory card, and I fold it in half, revealing USB contacts. I just stick it in the computer, offload the pictures, put it right back in the camera. I never have to lose anything. Here\\'s another example. Chris, you\\'re the source of all power. Will you be my power plug? Chris Anderson: Oh yeah. DP: Hold that and don\\'t let go. You might\\'ve seen this, this is Apple\\'s new laptop. This the power cord. It hooks on like this. And I\\'m sure every one of you has done this at some point in your lives, or one of your children. You walk along — and I\\'m about to pull this onto the floor. I don\\'t care. It\\'s a loaner. Here we go. Whoa! It\\'s magnetic — it doesn\\'t pull the laptop onto the floor. (Applause) In my very last example — I do a lot of my work using speech recognition software. And I\\'ll just — you have to be kind of quiet because the software is nervous. Speech recognition software is really great for doing emails very quickly; period. Like, I get hundreds of them a day; period. And it\\'s not just what I dictate that it writes down; period. I also use this feature called voice macros; period. Correct \"dissuade.\" Not \"just.\" Ok, this is not an ideal situation, because it\\'s getting the echo from the hall and stuff. The point is, I can respond to people very quickly by saying a short word, and having it write out a much longer thing. So if somebody sends me a fan letter, I\\'ll say, \"Thanks for that.\" [Thank you so much for taking the time to write ...] (Laughter) (Applause) And conversely, if somebody sends me hate mail — which happens daily — I say, \"Piss off.\" (Laughter) [I admire your frankness ...] (Laughter) (Applause) So that\\'s my dirty little secret. Don\\'t tell anyone. (Laughter) So the point is — this is a really interesting story. This is version eight of this software, and do you know what they put in version eight? No new features. It\\'s never happened before in software! The company put no new features. They just said, \"We\\'ll make this software work right.\" Right? Because for years, people had bought this software, tried it out — 95 percent accuracy was all they got, which means one in 20 words is wrong — and they\\'d put it in their drawer. And the company got sick of that, so they said, \"This version, we\\'re not going to do anything, but make sure it\\'s darned accurate.\" And so that\\'s what they did. This cult of doing things right is starting to spread. So, my final advice for those of you who are consumers of this technology: remember, if it doesn\\'t work, it\\'s not necessarily you, ok? It could be the design of the thing you\\'re using. Be aware in life of good design and bad design. And if you\\'re among the people who create this stuff: Easy is hard. Pre-sweat the details for your audience. Count the taps. Remember, the hard part is not deciding what features to add, it\\'s deciding what to leave out. And best of all, your motivation is: simplicity sells. CA: Bravo. DP: Thank you very much. CA: Hear, hear! (Applause)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tamaño de contexto.\n",
        "max_context_size = 100\n",
        "\n",
        "# Vocabulario como conjunto único de caracteres que existe en todo el texto.\n",
        "full_text = '. '.join(text).lower()\n",
        "chars_vocab = set(full_text)\n",
        "len(chars_vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLdTOTGHvuC-",
        "outputId": "76476237-083d-4a91-a26e-3ba57e3337c4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "56"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Diccionarios que asignan índices a caracteres y viceversa.\n",
        "# El diccionario `char2idx` servirá como tokenizador.\n",
        "char2idx = {k: v for v, k in enumerate(chars_vocab)}\n",
        "idx2char = {v: k for k, v in char2idx.items()}"
      ],
      "metadata": {
        "id": "OnRdcZshwrC3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizar"
      ],
      "metadata": {
        "id": "qaRsRsb8ycYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenización del texto completo\n",
        "tokenized_text = [char2idx[ch] for ch in full_text]\n",
        "tokenized_text[:10]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8e-l5awyeEZ",
        "outputId": "276d6986-a421-4c3d-87e9-a12a5704d0d9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[52, 14, 11, 43, 13, 3, 34, 47, 32, 30]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dividir el dataset"
      ],
      "metadata": {
        "id": "kJs48neuy7tX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separar el dataset entre entrenamiento y validación.\n",
        "# `p_val` será la proporción del corpus que se reservará para validación.\n",
        "# `num_val` es la cantidad de secuencias de tamaño `max_context_size` que se usará en validación.\n",
        "p_val = 0.1\n",
        "num_val = int(np.ceil(len(tokenized_text) * p_val / max_context_size))\n",
        "\n",
        "# Separar la porción de texto utilizada en entrenamiento de la de validación.\n",
        "train_text = tokenized_text[:-num_val * max_context_size]\n",
        "val_text = tokenized_text[-num_val * max_context_size:]\n",
        "\n",
        "tokenized_sentences_val = [val_text[init * max_context_size:init * (max_context_size + 1)] for init in range(num_val)]\n",
        "tokenized_sentences_train = [train_text[init:init + max_context_size] for init in range(len(train_text) - max_context_size + 1)]\n",
        "\n",
        "X = np.array(tokenized_sentences_train[:-1])\n",
        "y = np.array(tokenized_sentences_train[1:])\n",
        "\n",
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKY4GBMYy_F3",
        "outputId": "eac1f59e-34e7-4d38-bbfb-4cb95b9156be"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(379674, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X[0, :10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbzPPLVxzs6f",
        "outputId": "6f363bd4-c918-47a5-fd5e-94d71a9c639b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([52, 14, 11, 43, 13,  3, 34, 47, 32, 30])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_xBQw8yH73x",
        "outputId": "48a33119-a2fd-4751-83ec-a6e43a65e09b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(379674, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y[0, :10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GK7ANCD8zvEw",
        "outputId": "8b911709-99ca-4401-add5-5acd443e9127"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([14, 11, 43, 13,  3, 34, 47, 32, 30, 21])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(chars_vocab)\n",
        "vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqVs0SLFzzeJ",
        "outputId": "3b0c2fa7-bf96-43be-cffa-c63e2b99d22c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "56"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Definir el modelo"
      ],
      "metadata": {
        "id": "gQqba9L4z65X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(TimeDistributed(CategoryEncoding(num_tokens=vocab_size, output_mode='one_hot'), input_shape=(None, 1)))\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop')\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUopv_lyz_Al",
        "outputId": "e7054503-7f2f-441b-a850-ffc81e00afb2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " time_distributed (TimeDist  (None, None, 56)          0         \n",
            " ributed)                                                        \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, None, 100)         62800     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, None, 100)         0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, None, 56)          5656      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 68456 (267.41 KB)\n",
            "Trainable params: 68456 (267.41 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Definir como callback a la métrica perplexity"
      ],
      "metadata": {
        "id": "jgrT40ri0m2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PplCallback(keras.callbacks.Callback):\n",
        "    '''\n",
        "    Este callback es una solución ad-hoc para calcular al final de cada epoch de\n",
        "    entrenamiento la métrica de Perplejidad sobre un conjunto de datos de validación.\n",
        "    La perplejidad es una métrica cuantitativa para evaluar la calidad de la generación de secuencias.\n",
        "    Además implementa la finalización del entrenamiento (Early Stopping)\n",
        "    si la perplejidad no mejora después de `patience` epochs.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, val_data, history_ppl, patience=5):\n",
        "      # El callback lo inicializamos con secuencias de validación sobre las cuales\n",
        "      # mediremos la perplejidad y la paciencia para detener el entrenamiento.\n",
        "      self.val_data = val_data\n",
        "\n",
        "      self.target = []\n",
        "      self.padded = []\n",
        "\n",
        "      count = 0\n",
        "      self.info = []\n",
        "      self.history_ppl = history_ppl\n",
        "      self.min_score = np.inf\n",
        "      self.patience_counter = 0\n",
        "      self.patience = patience\n",
        "\n",
        "      # nos movemos en todas las secuencias de los datos de validación\n",
        "      for seq in self.val_data:\n",
        "\n",
        "        len_seq = len(seq)\n",
        "        # armamos todas las subsecuencias\n",
        "        subseq = [seq[:i] for i in range(len_seq)]\n",
        "        self.target.extend([seq[i] for i in range(len_seq)])\n",
        "\n",
        "        if len(subseq) != 0:\n",
        "\n",
        "          self.padded.append(pad_sequences(subseq, maxlen=max_context_size, padding='pre'))\n",
        "\n",
        "          self.info.append((count,count+len_seq))\n",
        "          count += len_seq\n",
        "\n",
        "      self.padded = np.vstack(self.padded)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "\n",
        "        # en `scores` iremos guardando la perplejidad de cada secuencia\n",
        "        scores = []\n",
        "\n",
        "        predictions = self.model.predict(self.padded, verbose=0)\n",
        "\n",
        "        # para cada secuencia de validación\n",
        "        for start,end in self.info:\n",
        "\n",
        "          # en `probs` iremos guardando las probabilidades de los términos target\n",
        "          probs = [predictions[idx_seq,-1,idx_vocab] for idx_seq, idx_vocab in zip(range(start, end), self.target[start:end])]\n",
        "\n",
        "          # calculamos la perplejidad por medio de logaritmos\n",
        "          scores.append(np.exp(-np.sum(np.log(probs))/(end-start)))\n",
        "\n",
        "        # promediamos todos los scores e imprimimos el valor promedio\n",
        "        current_score = np.mean(scores)\n",
        "        self.history_ppl.append(current_score)\n",
        "        print(f'\\n mean perplexity: {current_score} \\n')\n",
        "\n",
        "        # chequeamos si tenemos que detener el entrenamiento\n",
        "        if self.patience is not None:\n",
        "            if current_score < self.min_score:\n",
        "                self.min_score = current_score\n",
        "                self.model.save('my_model')\n",
        "                print('Saved new model!')\n",
        "                self.patience_counter = 0\n",
        "            else:\n",
        "                self.patience_counter += 1\n",
        "                if self.patience_counter == self.patience:\n",
        "                    print('Stopping training...')\n",
        "                    self.model.stop_training = True"
      ],
      "metadata": {
        "id": "btXyVA2y0nrd"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entrenamiento"
      ],
      "metadata": {
        "id": "2HxDc0iK0skq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history_ppl = []\n",
        "hist = model.fit(\n",
        "    X,\n",
        "    y,\n",
        "    epochs=20,\n",
        "    callbacks=[PplCallback(tokenized_sentences_val, history_ppl, patience=4)], batch_size=256)"
      ],
      "metadata": {
        "id": "MJ4WKhRW0uLz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00e4cd48-9583-420b-8cb8-2fc215cd626c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1484/1484 [==============================] - ETA: 0s - loss: 2.4028\n",
            " mean perplexity: 12.493498715914232 \n",
            "\n",
            "Saved new model!\n",
            "1484/1484 [==============================] - 865s 581ms/step - loss: 2.4028\n",
            "Epoch 2/20\n",
            "1484/1484 [==============================] - ETA: 0s - loss: 2.0617\n",
            " mean perplexity: 10.221945122426831 \n",
            "\n",
            "Saved new model!\n",
            "1484/1484 [==============================] - 870s 586ms/step - loss: 2.0617\n",
            "Epoch 3/20\n",
            "1484/1484 [==============================] - ETA: 0s - loss: 1.9350\n",
            " mean perplexity: 9.294862987898503 \n",
            "\n",
            "Saved new model!\n",
            "1484/1484 [==============================] - 868s 585ms/step - loss: 1.9350\n",
            "Epoch 4/20\n",
            "1484/1484 [==============================] - ETA: 0s - loss: 1.8552\n",
            " mean perplexity: 10.1437874215622 \n",
            "\n",
            "1484/1484 [==============================] - 826s 557ms/step - loss: 1.8552\n",
            "Epoch 5/20\n",
            "1484/1484 [==============================] - ETA: 0s - loss: 1.7984\n",
            " mean perplexity: 11.045578885529128 \n",
            "\n",
            "1484/1484 [==============================] - 885s 597ms/step - loss: 1.7984\n",
            "Epoch 6/20\n",
            "1484/1484 [==============================] - ETA: 0s - loss: 1.7554\n",
            " mean perplexity: 7.394253228612094 \n",
            "\n",
            "Saved new model!\n",
            "1484/1484 [==============================] - 860s 580ms/step - loss: 1.7554\n",
            "Epoch 7/20\n",
            "1484/1484 [==============================] - ETA: 0s - loss: 1.7216\n",
            " mean perplexity: 6.761277349408553 \n",
            "\n",
            "Saved new model!\n",
            "1484/1484 [==============================] - 806s 543ms/step - loss: 1.7216\n",
            "Epoch 8/20\n",
            "1484/1484 [==============================] - ETA: 0s - loss: 1.6937\n",
            " mean perplexity: 6.890705879936061 \n",
            "\n",
            "1484/1484 [==============================] - 807s 544ms/step - loss: 1.6937\n",
            "Epoch 9/20\n",
            "1484/1484 [==============================] - ETA: 0s - loss: 1.6708\n",
            " mean perplexity: 7.474062549551534 \n",
            "\n",
            "1484/1484 [==============================] - 790s 532ms/step - loss: 1.6708\n",
            "Epoch 10/20\n",
            "1484/1484 [==============================] - ETA: 0s - loss: 1.6511\n",
            " mean perplexity: 7.884732325599528 \n",
            "\n",
            "1484/1484 [==============================] - 844s 569ms/step - loss: 1.6511\n",
            "Epoch 11/20\n",
            "1484/1484 [==============================] - ETA: 0s - loss: 1.6348\n",
            " mean perplexity: 7.387924428952215 \n",
            "\n",
            "Stopping training...\n",
            "1484/1484 [==============================] - 843s 568ms/step - loss: 1.6348\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epoch_count = range(1, len(history_ppl) + 1)\n",
        "sns.lineplot(x=epoch_count, y=history_ppl)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "J1LWFLb14QrM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "outputId": "48caf03c-e4a4-4132-d374-6991e4d28b32"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generación de secuencias"
      ],
      "metadata": {
        "id": "8ENHmi804R9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_seq(model, seed_text, max_length, n_words):\n",
        "    '''\n",
        "        Exec model sequence prediction\n",
        "\n",
        "        Args:\n",
        "            model (keras): modelo entrenado\n",
        "            seed_text (string): texto de entrada (input_seq)\n",
        "            max_length (int): máxima longitud de la sequencia de entrada\n",
        "            n_words (int): números de caracteres a agregar a la sequencia de entrada\n",
        "        returns:\n",
        "            output_text (string): sentencia con las \"n_words\" agregadas\n",
        "    '''\n",
        "    output_text = seed_text\n",
        "\t# generate a fixed number of words\n",
        "    for _ in range(n_words):\n",
        "\t\t# Encodeamos\n",
        "        encoded = [char2idx[ch] for ch in output_text.lower()]\n",
        "\t\t# Si tienen distinto largo\n",
        "        encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
        "\n",
        "\t\t# Predicción softmax\n",
        "        y_hat = np.argmax(model.predict(encoded, verbose=0)[0, -1, :])\n",
        "\t\t# Vamos concatenando las predicciones\n",
        "        out_word = ''\n",
        "\n",
        "        out_word = idx2char[y_hat]\n",
        "\n",
        "\t\t# Agrego las palabras a la frase predicha\n",
        "        output_text += out_word\n",
        "    return output_text"
      ],
      "metadata": {
        "id": "yxGIv9n_4VBP"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = 'computers hav'\n",
        "generate_seq(model, input_text, max_length=max_context_size, n_words=30)"
      ],
      "metadata": {
        "id": "m13rAH3E4fJV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c46c4ffc-966c-4ceb-e085-02983559f339"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'computers have the simple the thing the thi'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Beam search y muestreo aleatorio"
      ],
      "metadata": {
        "id": "hGFbY5rH5QJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Funciones para hacer encoding y decoding.\n",
        "\n",
        "def encode(text, max_length=max_context_size):\n",
        "    encoded = [char2idx[ch] for ch in text]\n",
        "    encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
        "    return encoded\n",
        "\n",
        "def decode(seq):\n",
        "    return ''.join([idx2char[ch] for ch in seq])\n",
        "\n",
        "# función que selecciona candidatos para el beam search\n",
        "def select_candidates(pred,num_beams,vocab_size,history_probs,history_tokens,temp,mode):\n",
        "\n",
        "  # colectar todas las probabilidades para la siguiente búsqueda\n",
        "  pred_large = []\n",
        "\n",
        "  for idx,pp in enumerate(pred):\n",
        "    pred_large.extend(np.log(pp+1E-10)+history_probs[idx])\n",
        "\n",
        "  pred_large = np.array(pred_large)\n",
        "\n",
        "  # criterio de selección\n",
        "  if mode == 'det':\n",
        "    idx_select = np.argsort(pred_large)[::-1][:num_beams] # beam search determinista\n",
        "  elif mode == 'sto':\n",
        "    idx_select = np.random.choice(np.arange(pred_large.shape[0]), num_beams, p=softmax(pred_large/temp)) # beam search con muestreo aleatorio\n",
        "  else:\n",
        "    raise ValueError(f'Wrong selection mode. {mode} was given. det and sto are supported.')\n",
        "\n",
        "  # traducir a índices de token en el vocabulario\n",
        "  new_history_tokens = np.concatenate((np.array(history_tokens)[idx_select//vocab_size],\n",
        "                        np.array([idx_select%vocab_size]).T),\n",
        "                      axis=1)\n",
        "\n",
        "  # devolver el producto de las probabilidades (log) y la secuencia de tokens seleccionados\n",
        "  return pred_large[idx_select.astype(int)], new_history_tokens.astype(int)\n",
        "\n",
        "\n",
        "def beam_search(model, num_beams, num_words, input, temp=1, mode='det'):\n",
        "\n",
        "    # first iteration\n",
        "\n",
        "    # encode\n",
        "    encoded = encode(input)\n",
        "\n",
        "    # first prediction\n",
        "    y_hat = model.predict(encoded,verbose=0)[0, -1, :]\n",
        "\n",
        "    # get vocabulary size\n",
        "    vocab_size = y_hat.shape[0]\n",
        "\n",
        "    # initialize history\n",
        "    history_probs = [0] * num_beams\n",
        "    history_tokens = [encoded[0]] * num_beams\n",
        "\n",
        "    # select num_beams candidates\n",
        "    history_probs, history_tokens = select_candidates([y_hat],\n",
        "                                        num_beams,\n",
        "                                        vocab_size,\n",
        "                                        history_probs,\n",
        "                                        history_tokens,\n",
        "                                        temp,\n",
        "                                        mode)\n",
        "\n",
        "    # beam search loop\n",
        "    for i in range(num_words-1):\n",
        "\n",
        "      preds = []\n",
        "\n",
        "      for hist in history_tokens:\n",
        "\n",
        "        # actualizar secuencia de tokens\n",
        "        input_update = np.array([hist[i+1:]]).copy()\n",
        "\n",
        "        # predicción\n",
        "        y_hat = model.predict(input_update,verbose=0)[0,-1,:]\n",
        "\n",
        "        preds.append(y_hat)\n",
        "\n",
        "      history_probs, history_tokens = select_candidates(preds,\n",
        "                                                        num_beams,\n",
        "                                                        vocab_size,\n",
        "                                                        history_probs,\n",
        "                                                        history_tokens,\n",
        "                                                        temp,\n",
        "                                                        mode)\n",
        "\n",
        "    return history_tokens"
      ],
      "metadata": {
        "id": "65sd32365Pfe"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicción con beam search\n",
        "salidas = beam_search(model, num_beams=10, num_words=20, input='when i was a child')"
      ],
      "metadata": {
        "id": "G7sF6PMq5tb2"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "salidas[0]"
      ],
      "metadata": {
        "id": "vIZzphsX5zEL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be2895fe-4c63-4830-e13e-bcff3b82f80e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 28, 21, 25,\n",
              "       35, 47, 13, 47, 28, 10, 43, 47, 10, 47,  3, 21, 13, 17, 31, 29, 25,\n",
              "       35, 47, 13, 35, 47, 30, 21, 25, 47,  3,  0, 14, 54, 11, 30, 25, 29,\n",
              "       47])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode de las salidas.\n",
        "decode(salidas[0])"
      ],
      "metadata": {
        "id": "sN-r1P-k5zix",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "020be9e7-0aff-4347-c831-5e062a00be87"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'oooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooowhen i was a children in the computer '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    }
  ]
}